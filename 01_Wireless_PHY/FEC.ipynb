{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb5d561",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Introduction to Turbo Codes**\n",
    "\n",
    "Turbo codes, introduced by Claude Berrou in 1993, were the first practical channel codes to approach the **Shannon capacity limit**. Their breakthrough came from combining two ideas:\n",
    "\n",
    "* **Parallel concatenation of two convolutional encoders**\n",
    "* **Iterative decoding with soft information exchange**\n",
    "\n",
    "They are called “**turbo**” due to the **feedback-like decoding loop**, reminiscent of a turbo engine’s feedback mechanism.\n",
    "\n",
    "\n",
    "## **Basic Structure**\n",
    "\n",
    "Turbo coding consists of:\n",
    "\n",
    "### a. **Two Recursive Systematic Convolutional (RSC) Encoders**\n",
    "\n",
    "* Connected in parallel\n",
    "* Both encode the same input, but the second encoder sees an **interleaved** version of the input.\n",
    "\n",
    "### b. **Interleaver**\n",
    "\n",
    "* A deterministic or pseudorandom permutation of the input bits\n",
    "* Introduces randomness to break low-weight input patterns (improves minimum distance)\n",
    "\n",
    "### c. **Code Rate**\n",
    "\n",
    "* The basic configuration transmits:\n",
    "\n",
    "  * The **original input bits** (systematic)\n",
    "  * One **parity bit** from each encoder\n",
    "* Code rate:\n",
    "\n",
    "  $$\n",
    "  R = \\frac{\\text{Number of input bits}}{\\text{Number of transmitted bits}} = \\frac{k}{k + 2k} = \\frac{1}{3}\n",
    "  $$\n",
    "\n",
    "  (Puncturing can increase it to 1/2, 2/3, etc.)\n",
    "\n",
    "\n",
    "## **Encoding Process (Step-by-Step)**\n",
    "\n",
    "1. Input data: $\\mathbf{u} = [u_0, u_1, \\dots, u_{k-1}]$\n",
    "2. Encoder 1 takes $\\mathbf{u}$ and produces parity bits $\\mathbf{p}_1$\n",
    "3. Interleaver scrambles $\\mathbf{u} \\rightarrow \\mathbf{u}'$\n",
    "4. Encoder 2 takes $\\mathbf{u}'$ and produces parity bits $\\mathbf{p}_2$\n",
    "5. Transmitted bits:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{x} = \\left[ u_0, p_{1,0}, p_{2,0}, u_1, p_{1,1}, p_{2,1}, \\dots \\right]\n",
    "   $$\n",
    "\n",
    "\n",
    "## **Recursive Systematic Convolutional (RSC) Encoder**\n",
    "\n",
    "Each encoder is usually a **rate-1/2 RSC code**, implemented via:\n",
    "\n",
    "* Shift registers\n",
    "* Feedback and feedforward polynomials\n",
    "\n",
    "For example, a **constraint length 3** encoder might use:\n",
    "\n",
    "* Generator polynomials: $G_1 = 1 + D + D^2$, $G_2 = 1 + D^2$\n",
    "\n",
    "These generate both:\n",
    "\n",
    "* **Systematic bit**: Original input bit\n",
    "* **Parity bit**: Based on current and previous input bits\n",
    "\n",
    "\n",
    "## **Turbo Decoder (Iterative Decoding)**\n",
    "\n",
    "Turbo decoding is performed iteratively using **two Soft-Input Soft-Output (SISO)** decoders:\n",
    "\n",
    "### Step-by-Step:\n",
    "\n",
    "1. Each decoder uses the **Log-MAP algorithm** or **Max-Log-MAP** to decode its stream, taking into account **a priori probabilities**.\n",
    "2. Decoder 1 processes the systematic bits and its parity bits → produces **extrinsic LLRs**.\n",
    "3. Interleaver permutes these extrinsic values → fed into Decoder 2.\n",
    "4. Decoder 2 processes interleaved systematic bits and parity → updates extrinsic LLRs.\n",
    "5. These are **deinterleaved** and passed back to Decoder 1.\n",
    "\n",
    "Each iteration improves reliability of LLRs. After $N$ iterations, a **hard decision** is made:\n",
    "\n",
    "$$\n",
    "\\hat{u}_i = \\begin{cases}\n",
    "1 & \\text{if } L(u_i) > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "## **Mathematical Details of Decoding**\n",
    "\n",
    "The **Log-MAP algorithm** computes the **log-likelihood ratio (LLR)** for each bit:\n",
    "\n",
    "$$\n",
    "L(u_i) = \\log \\frac{P(u_i = 1 | y)}{P(u_i = 0 | y)}\n",
    "$$\n",
    "\n",
    "The LLR is decomposed as:\n",
    "\n",
    "$$\n",
    "L(u_i) = L_c(y_i) + L_{a}(u_i) + L_e(u_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $L_c(y_i)$: channel LLR from received symbols\n",
    "* $L_a(u_i)$: a priori info from other decoder\n",
    "* $L_e(u_i)$: extrinsic info passed to other decoder\n",
    "\n",
    "\n",
    "## **Performance and Properties**\n",
    "\n",
    "| Property             | Turbo Code                           |\n",
    "| -------------------- | ------------------------------------ |\n",
    "| Capacity-Approaching | Yes                                  |\n",
    "| Good at              | Moderate blocklength (1k–10k)        |\n",
    "| Drawback             | Error floor at high SNR              |\n",
    "| Complexity           | Medium-high due to iterations        |\n",
    "| Used in              | 3G, 4G LTE data channels, deep space |\n",
    "\n",
    "\n",
    "## **Turbo Code vs. Block Code**\n",
    "\n",
    "Turbo codes achieve high performance by:\n",
    "\n",
    "* **Interleaving**, which increases effective free distance\n",
    "* **Iterative decoding**, improving reliability per iteration\n",
    "* **Soft information exchange**, unlike traditional block codes\n",
    "\n",
    "\n",
    "## **Diagram Summary**\n",
    "\n",
    "```\n",
    "Input:  u0 u1 u2 u3 ...\n",
    "\n",
    "  ┌──────────────┐\n",
    "  │   Encoder 1  │─────→ parity 1\n",
    "  └──────────────┘\n",
    "        │\n",
    "        │ systematic bits → TX\n",
    "        ▼\n",
    "  ┌──────────────┐\n",
    "  │  Interleaver │──→ permuted input\n",
    "  └──────────────┘\n",
    "        ▼\n",
    "  ┌──────────────┐\n",
    "  │   Encoder 2  │─────→ parity 2\n",
    "  └──────────────┘\n",
    "```\n",
    "\n",
    "At receiver:\n",
    "\n",
    "* Decoder 1 ⇄ Decoder 2 (via interleaver/deinterleaver)\n",
    "* Iteratively exchange **LLRs**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5ad72",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Custom Turbo Code Implementation \n",
    "\n",
    "\n",
    "### 1. **Recursive Systematic Convolutional (RSC) Encoder**\n",
    "\n",
    "* Constraint length $K = 3$\n",
    "* Generator polynomials: $G_1 = (1, 1, 1)$, $G_2 = (1, 0, 1)$\n",
    "\n",
    "    * $G_1$ is the systematic path\n",
    "    * $G_2$ is the feedback parity path\n",
    "\n",
    "### 2. **Turbo Encoder**\n",
    "\n",
    "* Two identical RSC encoders\n",
    "* One processes original bits\n",
    "* The other processes interleaved bits\n",
    "\n",
    "### 3. **Turbo Decoder (Iterative)**\n",
    "\n",
    "* A basic version of **Soft-In Soft-Out (SISO)** decoding using **Log-MAP** (or approximation)\n",
    "* Exchange **extrinsic information** between the two decoders\n",
    "* Use **a priori + channel + extrinsic** LLRs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f44357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         [0 1 1 0 1 1 1 1 1 1]\n",
      "Systematic:    [0 1 1 0 1 1 1 1 1 1]\n",
      "Parity:        [0 1 0 0 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# ===========================\n",
    "# 1. Recursive Systematic Convolutional Encoder (RSC)\n",
    "# ===========================\n",
    "class RSCEncoder:\n",
    "    def __init__(self, g_sys, g_parity):\n",
    "        \"\"\"\n",
    "        g_sys, g_parity: generator polynomials as lists of bits, e.g., [1, 1, 1]\n",
    "        \"\"\"\n",
    "        self.g_sys = np.array(g_sys)\n",
    "        self.g_parity = np.array(g_parity)\n",
    "        self.K = len(g_sys)\n",
    "\n",
    "    def encode(self, bits):\n",
    "        \"\"\"\n",
    "        Encodes a bit array using a rate-1/2 RSC encoder.\n",
    "        Returns: (systematic_bits, parity_bits)\n",
    "        \"\"\"\n",
    "        state = np.zeros(self.K - 1, dtype=int)\n",
    "        sys_bits = []\n",
    "        parity_bits = []\n",
    "\n",
    "        for bit in bits:\n",
    "            sys_bits.append(bit)\n",
    "\n",
    "            # Compute feedback = input XOR feedback terms\n",
    "            feedback = bit\n",
    "            for i in range(1, self.K):\n",
    "                feedback ^= self.g_sys[i] & state[i - 1]\n",
    "\n",
    "            # Compute parity = feedback XOR parity terms\n",
    "            parity = feedback * self.g_parity[0]\n",
    "            for i in range(1, self.K):\n",
    "                parity ^= self.g_parity[i] & state[i - 1]\n",
    "            parity_bits.append(parity)\n",
    "\n",
    "            # Update shift register\n",
    "            state = np.roll(state, shift=1)\n",
    "            state[0] = feedback\n",
    "\n",
    "        return np.array(sys_bits), np.array(parity_bits)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    test_bits = np.random.randint(0, 2, 10)\n",
    "    encoder = RSCEncoder(g_sys=[1, 1, 1], g_parity=[1, 0, 1])\n",
    "    sys, parity = encoder.encode(test_bits)\n",
    "\n",
    "    print(\"Input:        \", test_bits)\n",
    "    print(\"Systematic:   \", sys)\n",
    "    print(\"Parity:       \", parity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c3a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Bits:    [0 1 1 0 1 0]\n",
      "Interleaved:      [1 0 1 0 1 0]\n",
      "Deinterleaved:    [0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 2. Interleaver / Deinterleaver\n",
    "# ===========================\n",
    "class Interleaver:\n",
    "    def __init__(self, pattern):\n",
    "        \"\"\"\n",
    "        pattern: a permutation of indices for interleaving (e.g., np.random.permutation(N))\n",
    "        \"\"\"\n",
    "        self.pattern = np.array(pattern)\n",
    "        self.inverse_pattern = np.argsort(self.pattern)\n",
    "\n",
    "    def interleave(self, bits):\n",
    "        return bits[self.pattern]\n",
    "\n",
    "    def deinterleave(self, bits):\n",
    "        return bits[self.inverse_pattern]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bits = np.array([0, 1, 1, 0, 1, 0])\n",
    "    pattern = np.random.permutation(len(bits))\n",
    "    interleaver = Interleaver(pattern)\n",
    "\n",
    "    interleaved = interleaver.interleave(bits)\n",
    "    recovered = interleaver.deinterleave(interleaved)\n",
    "\n",
    "    print(\"Original Bits:   \", bits)\n",
    "    print(\"Interleaved:     \", interleaved)\n",
    "    print(\"Deinterleaved:   \", recovered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc0399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Bits:        [0 1 1 0 1 1 1 1 1 1]\n",
      "Systematic Bits:  [0 1 1 0 1 1 1 1 1 1]\n",
      "Parity 1:         [0 1 0 0 0 1 1 0 1 1]\n",
      "Parity 2:         [0 1 0 1 1 1 0 0 0 0]\n",
      "Interleaved Bits: [0 1 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 3. Turbo Encoder\n",
    "# ===========================\n",
    "\n",
    "class TurboEncoder:\n",
    "    def __init__(self, g_sys, g_parity, interleaver):\n",
    "        \"\"\"\n",
    "        g_sys, g_parity: generator polynomials\n",
    "        interleaver: instance of Interleaver\n",
    "        \"\"\"\n",
    "        self.encoder1 = RSCEncoder(g_sys, g_parity)\n",
    "        self.encoder2 = RSCEncoder(g_sys, g_parity)\n",
    "        self.interleaver = interleaver\n",
    "\n",
    "    def encode(self, info_bits):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            sys_bits: Systematic bits (from encoder 1)\n",
    "            parity1: Parity bits from encoder 1\n",
    "            parity2: Parity bits from encoder 2 (interleaved input)\n",
    "            interleaved_bits: Interleaved version of info_bits\n",
    "        \"\"\"\n",
    "        sys_bits, parity1 = self.encoder1.encode(info_bits)\n",
    "        interleaved_bits = self.interleaver.interleave(info_bits)\n",
    "        _, parity2 = self.encoder2.encode(interleaved_bits)\n",
    "        return sys_bits, parity1, parity2, interleaved_bits\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    info_bits = np.random.randint(0, 2, 10)\n",
    "    interleaver_pattern = np.random.permutation(len(info_bits))\n",
    "    interleaver = Interleaver(interleaver_pattern)\n",
    "\n",
    "    encoder = TurboEncoder(g_sys=[1, 1, 1], g_parity=[1, 0, 1], interleaver=interleaver)\n",
    "    sys, p1, p2, interleaved = encoder.encode(info_bits)\n",
    "\n",
    "    print(\"Info Bits:       \", info_bits)\n",
    "    print(\"Systematic Bits: \", sys)\n",
    "    print(\"Parity 1:        \", p1)\n",
    "    print(\"Parity 2:        \", p2)\n",
    "    print(\"Interleaved Bits:\", interleaved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f143ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 4. Log-MAP approximation for SISO Decoder (max-log-MAP)\n",
    "# ===========================\n",
    "class SISODecoder:\n",
    "    def __init__(self, g_sys, g_parity, noise_var):\n",
    "        self.noise_var = noise_var\n",
    "        self.states = 4  # for constraint length 3\n",
    "        self.trellis = self._build_trellis(g_sys, g_parity)\n",
    "\n",
    "    def _build_trellis(self, g_sys, g_parity):\n",
    "        # Build manually for constraint length = 3\n",
    "        trellis = {}\n",
    "        for s in range(4):\n",
    "            for u in [0, 1]:\n",
    "                current_state = list(map(int, np.binary_repr(s, width=2)))\n",
    "                input_bit = u\n",
    "\n",
    "                # Feedback (RSC)\n",
    "                fb = input_bit ^ (g_sys[1] & current_state[0]) ^ (g_sys[2] & current_state[1])\n",
    "                parity = fb ^ (g_parity[1] & current_state[0]) ^ (g_parity[2] & current_state[1])\n",
    "\n",
    "                next_state = (fb << 1) | current_state[0]\n",
    "                next_state &= 0b11  # 2-bit state\n",
    "                trellis[(s, u)] = (next_state, parity)\n",
    "        return trellis\n",
    "\n",
    "    def decode(self, y_sys, y_parity, apriori):\n",
    "        N = len(y_sys)\n",
    "        alpha = np.full((N + 1, 4), -np.inf)\n",
    "        beta = np.full((N + 1, 4), -np.inf)\n",
    "        alpha[0, 0] = 0\n",
    "        beta[N, :] = 0  # all states equally likely at the end\n",
    "\n",
    "        # Forward alpha\n",
    "        for k in range(N):\n",
    "            for s_prev in range(4):\n",
    "                for u in [0, 1]:\n",
    "                    s_next, parity = self.trellis[(s_prev, u)]\n",
    "                    metric = -0.5 / self.noise_var * (\n",
    "                        (2*u - 1 - y_sys[k])**2 + (2*parity - 1 - y_parity[k])**2\n",
    "                    ) + apriori[k]*u\n",
    "                    alpha[k+1, s_next] = max(alpha[k+1, s_next], alpha[k, s_prev] + metric)\n",
    "\n",
    "        # Backward beta\n",
    "        for k in reversed(range(N)):\n",
    "            for s_next in range(4):\n",
    "                for s_prev in range(4):\n",
    "                    for u in [0, 1]:\n",
    "                        if self.trellis.get((s_prev, u), (None,))[0] == s_next:\n",
    "                            parity = self.trellis[(s_prev, u)][1]\n",
    "                            metric = -0.5 / self.noise_var * (\n",
    "                                (2*u - 1 - y_sys[k])**2 + (2*parity - 1 - y_parity[k])**2\n",
    "                            ) + apriori[k]*u\n",
    "                            beta[k, s_prev] = max(beta[k, s_prev], beta[k+1, s_next] + metric)\n",
    "\n",
    "        # LLR output\n",
    "        extrinsic = np.zeros(N)\n",
    "        for k in range(N):\n",
    "            num = -np.inf\n",
    "            den = -np.inf\n",
    "            for s_prev in range(4):\n",
    "                for u in [0, 1]:\n",
    "                    s_next, parity = self.trellis[(s_prev, u)]\n",
    "                    metric = alpha[k, s_prev] + beta[k+1, s_next] + \\\n",
    "                             (-0.5 / self.noise_var * ((2*u - 1 - y_sys[k])**2 + (2*parity - 1 - y_parity[k])**2)) + \\\n",
    "                             apriori[k] * u\n",
    "                    if u == 1:\n",
    "                        num = max(num, metric)\n",
    "                    else:\n",
    "                        den = max(den, metric)\n",
    "            extrinsic[k] = num - den\n",
    "\n",
    "        return extrinsic\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499bfbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 5. Full Turbo Decoder (Iterative)\n",
    "# ===========================\n",
    "\n",
    "class TurboDecoder:\n",
    "    def __init__(self, g_sys, g_parity, interleaver, noise_var, n_iter=5):\n",
    "        \"\"\"\n",
    "        g_sys, g_parity: generator polynomials\n",
    "        interleaver: Interleaver instance\n",
    "        noise_var: noise variance of the AWGN channel\n",
    "        n_iter: number of decoding iterations\n",
    "        \"\"\"\n",
    "        self.interleaver = interleaver\n",
    "        self.siso1 = SISODecoder(g_sys, g_parity, noise_var)\n",
    "        self.siso2 = SISODecoder(g_sys, g_parity, noise_var)\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def decode(self, rx_sys, rx_p1, rx_p2):\n",
    "        \"\"\"\n",
    "        rx_sys: received noisy systematic (LLR)\n",
    "        rx_p1: received noisy parity1 (LLR)\n",
    "        rx_p2: received noisy parity2 (LLR)\n",
    "        Returns:\n",
    "            decoded_bits: final hard decision after n_iter iterations\n",
    "            final_llr: final LLRs from decoder 1\n",
    "        \"\"\"\n",
    "        N = len(rx_sys)\n",
    "        apriori1 = np.zeros(N)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            # Decoder 1\n",
    "            llr1 = self.siso1.decode(rx_sys, rx_p1, apriori1)\n",
    "            extrinsic1 = llr1 - apriori1\n",
    "            apriori2 = self.interleaver.interleave(extrinsic1)\n",
    "\n",
    "            # Decoder 2\n",
    "            inter_rx_sys = self.interleaver.interleave(rx_sys)\n",
    "            llr2 = self.siso2.decode(inter_rx_sys, rx_p2, apriori2)\n",
    "            extrinsic2 = llr2 - apriori2\n",
    "            apriori1 = self.interleaver.deinterleave(extrinsic2)\n",
    "\n",
    "        final_llr = llr1\n",
    "        decoded_bits = (final_llr >= 0).astype(int)\n",
    "        return decoded_bits, final_llr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25dfd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Step 6: AWGN simulation with BER measurement and plotting.\n",
    "# ===========================\n",
    "\n",
    "class TurboSimulation:\n",
    "    def __init__(self, g_sys, g_parity, block_len, n_iter=5):\n",
    "        self.g_sys = g_sys\n",
    "        self.g_parity = g_parity\n",
    "        self.block_len = block_len\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def bpsk_mod(self, bits):\n",
    "        return 1 - 2 * bits  # 0 → +1, 1 → -1\n",
    "\n",
    "    def run(self, snr_db, num_blocks=20):\n",
    "        snr_linear = 10 ** (snr_db / 10)\n",
    "        noise_var = 1 / (2 * snr_linear)\n",
    "        sigma = np.sqrt(noise_var)\n",
    "\n",
    "        total_errors = 0\n",
    "        total_bits = 0\n",
    "\n",
    "        for _ in range(num_blocks):\n",
    "            info_bits = np.random.randint(0, 2, self.block_len)\n",
    "            interleaver_pattern = np.random.permutation(self.block_len)\n",
    "            interleaver = Interleaver(interleaver_pattern)\n",
    "\n",
    "            encoder = TurboEncoder(self.g_sys, self.g_parity, interleaver)\n",
    "            sys_bits, p1_bits, p2_bits, _ = encoder.encode(info_bits)\n",
    "\n",
    "            tx_sys = self.bpsk_mod(sys_bits)\n",
    "            tx_p1 = self.bpsk_mod(p1_bits)\n",
    "            tx_p2 = self.bpsk_mod(p2_bits)\n",
    "\n",
    "            rx_sys = tx_sys + sigma * np.random.randn(self.block_len)\n",
    "            rx_p1  = tx_p1 + sigma * np.random.randn(self.block_len)\n",
    "            rx_p2  = tx_p2 + sigma * np.random.randn(self.block_len)\n",
    "\n",
    "            rx_sys_llr = 2 * rx_sys / noise_var\n",
    "            rx_p1_llr  = 2 * rx_p1  / noise_var\n",
    "            rx_p2_llr  = 2 * rx_p2  / noise_var\n",
    "\n",
    "            decoder = TurboDecoder(self.g_sys, self.g_parity, interleaver, noise_var, self.n_iter)\n",
    "            decoded_bits, _ = decoder.decode(rx_sys_llr, rx_p1_llr, rx_p2_llr)\n",
    "\n",
    "            total_errors += np.sum(decoded_bits != info_bits)\n",
    "            total_bits += self.block_len\n",
    "\n",
    "        ber = total_errors / total_bits\n",
    "        return ber\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9edbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sim = TurboSimulation(g_sys=[1,1,1], g_parity=[1,0,1], block_len=5000, n_iter=6)\n",
    "    snr_range = np.arange(0, 3.5, 0.5)\n",
    "    ber_values = [sim.run(snr_db,num_blocks = 30) for snr_db in snr_range]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.semilogy(snr_range, ber_values, marker='o')\n",
    "    plt.grid(True, which='both')\n",
    "    plt.title(\"Turbo Code BER vs. SNR (Max-Log-MAP, 6 Iterations)\")\n",
    "    plt.xlabel(\"SNR (dB)\")\n",
    "    plt.ylabel(\"Bit Error Rate (BER)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef07f4",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## **Polar codes** \n",
    "\n",
    "Invented by Erdal Arıkan in 2009, are the **first class of error-correcting codes** that are **mathematically proven to achieve the capacity** of binary-input symmetric memoryless channels (B-DMC), such as the Binary Erasure Channel (BEC) and AWGN.\n",
    "\n",
    "### Core Idea: Channel Polarization\n",
    "\n",
    "The fundamental idea behind polar codes is called **channel polarization**. Given a communication channel $W$, we construct $N = 2^n$ **synthesized channels** $W_1, W_2, ..., W_N$, such that:\n",
    "\n",
    "* Some channels become **almost noiseless** ($\\text{capacity} \\to 1$)\n",
    "* Others become **completely noisy** ($\\text{capacity} \\to 0$)\n",
    "\n",
    "We then:\n",
    "\n",
    "* Transmit data bits over the **good (reliable)** channels\n",
    "* Fix the inputs of **bad (unreliable)** channels to known values (called **frozen bits**)\n",
    "\n",
    "### Polar Encoding\n",
    "\n",
    "#### a. **Input Vector**\n",
    "\n",
    "Let:\n",
    "\n",
    "* $\\mathbf{u} = [u_0, u_1, \\dots, u_{N-1}]$: message + frozen bits\n",
    "* $N = 2^n$: block length\n",
    "\n",
    "#### b. **Polar Transform**\n",
    "\n",
    "Encoding is done via:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{u} \\cdot G_N\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $G_N = B_N \\cdot F^{\\otimes n}$\n",
    "* $F = \\begin{bmatrix}1 & 0 \\\\ 1 & 1\\end{bmatrix}$\n",
    "* $F^{\\otimes n}$ is the **n-th Kronecker power**\n",
    "* $B_N$: bit-reversal permutation matrix\n",
    "\n",
    "The polar transform mixes bits so that some synthesized channels become more reliable than others.\n",
    "\n",
    "\n",
    "### Frozen vs Information Bits\n",
    "\n",
    "Once channels are polarized:\n",
    "\n",
    "* Select **K** most reliable channels → assign **data bits**\n",
    "* Remaining $N - K$ → assign **frozen bits** (usually zero)\n",
    "\n",
    "These positions are **precomputed** based on channel type (e.g., AWGN) and SNR using **Bhattacharyya parameters** or **density evolution**.\n",
    "\n",
    "### Decoding: Successive Cancellation (SC)\n",
    "\n",
    "SC decoding proceeds **bit by bit**:\n",
    "\n",
    "$$\n",
    "\\hat{u}_i = \\begin{cases}\n",
    "0 & \\text{if } i \\text{ is frozen} \\\\\n",
    "\\arg\\max_{u_i} P(u_i | y, \\hat{u}_0^{i-1}) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Uses **LLR recursion**\n",
    "* Early decisions affect later ones\n",
    "* Very fast (complexity $O(N \\log N)$)\n",
    "\n",
    "### Successive Cancellation List (SCL) Decoding\n",
    "\n",
    "SC decoding suffers from:\n",
    "\n",
    "* Poor performance at short blocklengths\n",
    "* High error rate due to early decision errors\n",
    "\n",
    "**SCL decoding** improves performance by:\n",
    "\n",
    "* Keeping a **list of the best decoding paths**\n",
    "* Selecting the final codeword using a **CRC check** (used in 5G NR)\n",
    "\n",
    "SCL-CRC is the **standard decoding method** in real systems.\n",
    "\n",
    "\n",
    "### Example: $N = 8$ Polar Code\n",
    "\n",
    "* Block length $N = 8$\n",
    "* K = 4 data bits, 4 frozen bits\n",
    "* Frozen positions: {0, 1, 2, 4}\n",
    "* Information positions: {3, 5, 6, 7}\n",
    "\n",
    "place:\n",
    "\n",
    "* 0s in frozen positions\n",
    "* message bits in data positions\n",
    "\n",
    "compute:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{u} \\cdot G_8\n",
    "$$\n",
    "\n",
    "### Polar Code Properties\n",
    "\n",
    "| Feature      | Description                                      |\n",
    "| ------------ | ------------------------------------------------ |\n",
    "| Complexity   | $O(N \\log N)$ encoding & decoding                |\n",
    "| Capacity     | Achieves capacity for B-DMCs                     |\n",
    "| Performance  | Excellent with SCL-CRC (especially at large $N$) |\n",
    "| Block Length | Must be power of 2                               |\n",
    "| Error Floor  | Avoided using SCL decoding                       |\n",
    "\n",
    "---\n",
    "\n",
    "### Use in 5G NR\n",
    "\n",
    "* Polar codes are used for **control channels** in **5G NR**:\n",
    "\n",
    "  * **PBCH**, **PDCCH**, **PUCCH**, **DCI**\n",
    "* Data channels use **LDPC codes**\n",
    "\n",
    "Why Polar for control?\n",
    "\n",
    "* Low latency\n",
    "* Short block length efficiency\n",
    "* Efficient hardware implementation\n",
    "\n",
    "\n",
    "## Polar vs Turbo vs LDPC\n",
    "\n",
    "| Feature            | Polar               | Turbo           | LDPC               |\n",
    "| ------------------ | ------------------- | --------------- | ------------------ |\n",
    "| Invented           | 2009                | 1993            | 1960s / modernized |\n",
    "| Decoding           | SC, SCL             | Iterative (MAP) | Iterative (BP)     |\n",
    "| Capacity-achieving | Yes (B-DMC)         | Near capacity   | Yes (long block)   |\n",
    "| Used in 5G NR      | Control             | No              | Data (PDSCH)       |\n",
    "| Performance        | Excellent (SCL-CRC) | Moderate        | Excellent (long)   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b373dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "# ===============================\n",
    "# 1. Polar Transform Matrix\n",
    "# ===============================\n",
    "def polar_transform_matrix(N):\n",
    "    \"\"\"Generate the Polar transform matrix G_N = B_N * F^{⊗n}\"\"\"\n",
    "    def kronecker_power(F, n):\n",
    "        result = F\n",
    "        for _ in range(1, n):\n",
    "            result = np.kron(result, F)\n",
    "        return result\n",
    "\n",
    "    def bit_reversal(n):\n",
    "        N = 2 ** n\n",
    "        result = np.arange(N)\n",
    "        for i in range(N):\n",
    "            b = '{:0{width}b}'.format(i, width=n)\n",
    "            result[i] = int(b[::-1], 2)\n",
    "        return result\n",
    "\n",
    "    n = int(np.log2(N))\n",
    "    F = np.array([[1, 0], [1, 1]], dtype=int)\n",
    "    G = kronecker_power(F, n) % 2\n",
    "    bit_rev_order = bit_reversal(n)\n",
    "    G = G[bit_rev_order]\n",
    "    return G\n",
    "\n",
    "# ===============================\n",
    "# 2. Polar Encoder\n",
    "# ===============================\n",
    "def polar_encode(u, G_N):\n",
    "    \"\"\"Polar encoding: x = u * G_N (mod 2)\"\"\"\n",
    "    return np.mod(u @ G_N, 2)\n",
    "\n",
    "# ===============================\n",
    "# 3. Successive Cancellation Decoder (SC)\n",
    "# ===============================\n",
    "def sc_decode(llr, frozen_bits):\n",
    "    N = len(llr)\n",
    "    u_hat = np.zeros(N, dtype=int)\n",
    "\n",
    "    def recursive_decode(llr, depth=0):\n",
    "        n = len(llr)\n",
    "        if n == 1:\n",
    "            i = len(u_hat) - len(llr)\n",
    "            if frozen_bits[i] == 1:\n",
    "                return np.array([0])\n",
    "            else:\n",
    "                return np.array([0 if llr[0] >= 0 else 1])\n",
    "        else:\n",
    "            llr_left = np.sign(llr[:n//2]) * np.minimum(np.abs(llr[:n//2]), np.abs(llr[n//2:]))\n",
    "            llr_right = llr[n//2:] + ((1 - 2 * recursive_decode(llr_left, depth + 1)) * llr[:n//2])\n",
    "            left_bits = recursive_decode(llr_left, depth + 1)\n",
    "            right_bits = recursive_decode(llr_right, depth + 1)\n",
    "            return np.concatenate([(left_bits ^ right_bits), right_bits])\n",
    "\n",
    "    return recursive_decode(llr)\n",
    "\n",
    "# ===============================\n",
    "# 4. Test Case\n",
    "# ===============================\n",
    "# Parameters\n",
    "N = 8  # block length (must be power of 2)\n",
    "K = 4  # number of data bits\n",
    "np.random.seed(0)\n",
    "\n",
    "# Reliability order (for simplicity, manually selected for N=8 and AWGN)\n",
    "# In practice use Arikan's Bhattacharyya or 5G standards\n",
    "reliable_indices = [3, 5, 6, 7]  # information bits\n",
    "frozen_indices = [0, 1, 2, 4]    # frozen bits\n",
    "\n",
    "# Information bits\n",
    "info_bits = np.random.randint(0, 2, K)\n",
    "\n",
    "# Prepare u vector (with frozen bits = 0)\n",
    "u = np.zeros(N, dtype=int)\n",
    "u[reliable_indices] = info_bits\n",
    "\n",
    "# Encoding\n",
    "G_N = polar_transform_matrix(N)\n",
    "x = polar_encode(u, G_N)\n",
    "\n",
    "# BPSK modulation and AWGN\n",
    "def bpsk(x): return 1 - 2 * x\n",
    "def add_awgn(x, snr_db):\n",
    "    snr = 10 ** (snr_db / 10)\n",
    "    noise_std = np.sqrt(1 / (2 * snr))\n",
    "    return x + noise_std * np.random.randn(len(x))\n",
    "\n",
    "snr_db = 3\n",
    "x_mod = bpsk(x)\n",
    "y = add_awgn(x_mod, snr_db)\n",
    "llr = 2 * y * (10 ** (snr_db / 10))  # LLR for BPSK over AWGN\n",
    "\n",
    "# Frozen bit mask: 1 = frozen, 0 = info\n",
    "frozen_mask = np.ones(N, dtype=int)\n",
    "frozen_mask[reliable_indices] = 0\n",
    "\n",
    "# Decoding\n",
    "u_hat = sc_decode(llr, frozen_mask)\n",
    "decoded_info_bits = u_hat[reliable_indices]\n",
    "\n",
    "# Compute BER\n",
    "ber = np.sum(decoded_info_bits != info_bits) / K\n",
    "info_bits, decoded_info_bits, ber\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# BER vs SNR for Polar Code with SC Decoding\n",
    "# ===============================\n",
    "\n",
    "def simulate_polar_sc(snr_db, N=128, K=64, num_trials=100):\n",
    "    # Manually select reliability order (simplified)\n",
    "    reliable_indices = sorted(np.random.choice(np.arange(N), K, replace=False))\n",
    "    frozen_mask = np.ones(N, dtype=int)\n",
    "    frozen_mask[reliable_indices] = 0\n",
    "\n",
    "    G_N = polar_transform_matrix(N)\n",
    "    total_errors = 0\n",
    "    total_bits = 0\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        # Generate info bits and prepare u\n",
    "        info_bits = np.random.randint(0, 2, K)\n",
    "        u = np.zeros(N, dtype=int)\n",
    "        u[reliable_indices] = info_bits\n",
    "\n",
    "        # Encode\n",
    "        x = polar_encode(u, G_N)\n",
    "        x_mod = bpsk(x)\n",
    "        y = add_awgn(x_mod, snr_db)\n",
    "        llr = 2 * y * (10 ** (snr_db / 10))\n",
    "\n",
    "        # Decode\n",
    "        u_hat = sc_decode(llr, frozen_mask)\n",
    "        decoded_info = u_hat[reliable_indices]\n",
    "\n",
    "        total_errors += np.sum(decoded_info != info_bits)\n",
    "        total_bits += K\n",
    "\n",
    "    return total_errors / total_bits\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Successive Cancellation List (SCL) Decoder with CRC\n",
    "# ===============================\n",
    "\n",
    "def crc_check(bits, crc_poly, crc_len):\n",
    "    \"\"\"Check if CRC remainder is zero.\"\"\"\n",
    "    m = np.append(bits, np.zeros(crc_len, dtype=int))\n",
    "    poly = Polynomial(m[::-1])\n",
    "    divisor = Polynomial(crc_poly[::-1])\n",
    "    remainder = poly % divisor\n",
    "    return np.all(np.round(remainder.coef[::-1][:crc_len]) == 0)\n",
    "\n",
    "def generate_crc(bits, crc_poly, crc_len):\n",
    "    \"\"\"Append CRC to bits.\"\"\"\n",
    "    m = np.append(bits, np.zeros(crc_len, dtype=int))\n",
    "    poly = Polynomial(m[::-1])\n",
    "    divisor = Polynomial(crc_poly[::-1])\n",
    "    remainder = poly % divisor\n",
    "    crc = np.round(remainder.coef[::-1][:crc_len]).astype(int)\n",
    "    return np.append(bits, crc)\n",
    "\n",
    "def scl_decode(llr, frozen_mask, L=8, crc_poly=None, crc_len=0):\n",
    "    \"\"\"\n",
    "    SCL decoder with optional CRC.\n",
    "    \"\"\"\n",
    "    N = len(llr)\n",
    "    paths = [(np.zeros(N, dtype=int), 0.0)]  # (path, path_metric)\n",
    "\n",
    "    for i in range(N):\n",
    "        new_paths = []\n",
    "        for path, metric in paths:\n",
    "            if frozen_mask[i]:\n",
    "                new_path = path.copy()\n",
    "                new_path[i] = 0\n",
    "                new_metric = metric + np.log1p(np.exp(-abs(llr[i])))\n",
    "                new_paths.append((new_path, new_metric))\n",
    "            else:\n",
    "                for bit in [0, 1]:\n",
    "                    new_path = path.copy()\n",
    "                    new_path[i] = bit\n",
    "                    if bit == 0:\n",
    "                        new_metric = metric + np.log1p(np.exp(-abs(llr[i])))\n",
    "                    else:\n",
    "                        new_metric = metric + np.log1p(np.exp(abs(llr[i])))\n",
    "                    new_paths.append((new_path, new_metric))\n",
    "        paths = sorted(new_paths, key=lambda x: x[1])[:L]\n",
    "\n",
    "    # CRC check\n",
    "    if crc_poly is not None:\n",
    "        for path, _ in paths:\n",
    "            info_bits = path[frozen_mask == 0]\n",
    "            if crc_check(info_bits, crc_poly, crc_len):\n",
    "                return path\n",
    "        return paths[0][0]  # fallback\n",
    "    else:\n",
    "        return paths[0][0]\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Simulation with CRC + SCL\n",
    "# ==============================\n",
    "def simulate_polar_scl_crc_fixed_errors(\n",
    "    snr_db, N=128, K=64, crc_poly=[1,1,0,1], crc_len=3, L=8, target_errors=1000):\n",
    "    effective_K = K - crc_len\n",
    "    info_indices = sorted(np.random.choice(np.arange(N), K, replace=False))\n",
    "    frozen_mask = np.ones(N, dtype=int)\n",
    "    frozen_mask[info_indices] = 0\n",
    "\n",
    "    G_N = polar_transform_matrix(N)\n",
    "    total_errors = 0\n",
    "    total_bits = 0\n",
    "\n",
    "    while total_errors < target_errors:\n",
    "        bits = np.random.randint(0, 2, effective_K)\n",
    "        bits_with_crc = generate_crc(bits, crc_poly, crc_len)\n",
    "        u = np.zeros(N, dtype=int)\n",
    "        u[info_indices] = bits_with_crc\n",
    "\n",
    "        x = polar_encode(u, G_N)\n",
    "        x_mod = bpsk(x)\n",
    "        y = add_awgn(x_mod, snr_db)\n",
    "        llr = 2 * y * (10 ** (snr_db / 10))\n",
    "\n",
    "        u_hat = scl_decode(llr, frozen_mask, L=L, crc_poly=crc_poly, crc_len=crc_len)\n",
    "        decoded_info = u_hat[info_indices][:effective_K]\n",
    "\n",
    "        total_errors += np.sum(decoded_info != bits)\n",
    "        total_bits += effective_K\n",
    "\n",
    "    return total_errors / total_bits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb41fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ber_curve_scl_crc = [\n",
    "    simulate_polar_scl_crc_fixed_errors(snr, N=128, K=64, crc_len=3, L=8, target_errors=1000)\n",
    "    for snr in snr_range\n",
    "]\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(snr_range, ber_curve_scl_crc, marker='o', label='Polar SCL + CRC (L=8)')\n",
    "plt.grid(True, which='both')\n",
    "plt.title(\"Polar Code BER vs SNR with SCL + CRC (N=128, K=61 + CRC 3)\")\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"Bit Error Rate (BER)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dabdf0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "There are several Python libraries that provide implementations of **Turbo**, **LDPC**, and **Polar** codes. Below are the most relevant ones, from research-grade to production-quality:\n",
    "\n",
    "### **CommPy** (Communications with Python)\n",
    "\n",
    "* **GitHub**: [https://github.com/veeresht/CommPy](https://github.com/veeresht/CommPy)\n",
    "* **Status**: Actively used in academia (not full 5G spec compliant)\n",
    "* **Implements**:\n",
    "\n",
    "  * Turbo codes (basic RSC, iterative decoding)\n",
    "  * Convolutional codes (Viterbi)\n",
    "  * Basic LDPC (with `pyldpc` dependency)\n",
    "  * ❌ Polar (not included)\n",
    "\n",
    "---\n",
    "\n",
    "### **PyLDPC**\n",
    "\n",
    "* **GitHub**: [https://github.com/flennerhag/pyldpc](https://github.com/flennerhag/pyldpc)\n",
    "* **Implements**:\n",
    "\n",
    "  * LDPC: generation, encoding, belief propagation decoding\n",
    "* **Supports**:\n",
    "\n",
    "  * Regular/irregular LDPC\n",
    "  * Custom parity-check matrices\n",
    "\n",
    "> No Turbo or Polar support. Use for LDPC simulation only.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sionna** (NVIDIA) – 5G & Deep Learning Ready\n",
    "\n",
    "* **GitHub**: [https://github.com/NVIDIA/sionna](https://github.com/NVIDIA/sionna)\n",
    "* **Platform**: TensorFlow 2.x\n",
    "* **Implements**:\n",
    "\n",
    "  * Polar codes (including CRC-aided SC and SCL decoding)\n",
    "  * LDPC (3GPP 5G NR compliant)\n",
    "  * Turbo (basic support)\n",
    "  * 5G NR full PHY chain\n",
    "\n",
    "> If you're doing **5G** simulations or using **ML + comms**, this is the **most comprehensive and accurate** toolkit available.\n",
    "\n",
    "---\n",
    "\n",
    "### **IT++ (via Python bindings)**\n",
    "\n",
    "* **C++ library with Python wrappers**\n",
    "* Implements all major FEC codes\n",
    "* But setup is difficult, and usage is dated\n",
    "\n",
    "> Not recommended unless you already use it in C++\n",
    "\n",
    "---\n",
    "\n",
    "### **OpenFEC**\n",
    "\n",
    "* LDPC and Turbo codes (3GPP-like)\n",
    "* High-performance C-based implementation, hard to use directly in Python\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch Implementations\n",
    "\n",
    "Some GitHub repositories (e.g. TurboAE, PolarNet) offer differentiable implementations:\n",
    "\n",
    "* **TurboAE**: Turbo autoencoder-based neural decoder\n",
    "* **PolarNet**: Polar code SC/SCL decoding via deep learning\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33642193",
   "metadata": {},
   "source": [
    "# LDPC (Low-Density Parity-Check) codes\n",
    "\n",
    "\n",
    "\n",
    "## 1. Introduction and Motivation\n",
    "\n",
    "Low-Density Parity-Check (LDPC) codes are **linear block codes** defined by a sparse parity-check matrix. Proposed by Gallager in the 1960s and rediscovered in the 1990s, LDPC codes are known for:\n",
    "\n",
    "* Near-capacity performance on many channels.\n",
    "* Efficient iterative decoding via message-passing algorithms.\n",
    "* Practical use in standards like Wi-Fi, 5G, DVB-S2.\n",
    "\n",
    "\n",
    "## 2. Code Definition\n",
    "\n",
    "An LDPC code is a linear block code with parameters $(n, k)$, where:\n",
    "\n",
    "* **n**: codeword length\n",
    "* **k**: message length\n",
    "* **Rate**: $R = \\frac{k}{n}$\n",
    "\n",
    "It is defined by its **parity-check matrix** $H$:\n",
    "\n",
    "$$\n",
    "H \\cdot \\mathbf{c}^{\\top} = \\mathbf{0} \\quad \\text{over GF(2)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\mathbf{c}$ is a codeword of length $n$.\n",
    "* $H$ is an $(n - k) \\times n$ sparse binary matrix.\n",
    "\n",
    "**Low density** means that most entries in $H$ are zero.\n",
    "\n",
    "\n",
    "\n",
    "## 3. Parity-Check Matrix Properties\n",
    "\n",
    "A parity-check matrix $H$ is said to be **(d\\_v, d\\_c)-regular** if:\n",
    "\n",
    "* Each column has exactly $d_v$ ones.\n",
    "* Each row has exactly $d_c$ ones.\n",
    "\n",
    "**Irregular** LDPC codes allow variable column/row weights for improved performance.\n",
    "\n",
    "**Key property**:\n",
    "\n",
    "$$\n",
    "H \\cdot \\mathbf{c}^{\\top} = \\mathbf{0} \\implies \\text{each row of } H \\text{ imposes a parity-check constraint}.\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "For a row $h_i$:\n",
    "\n",
    "$$\n",
    "h_{i1} c_1 + h_{i2} c_2 + \\ldots + h_{in} c_n = 0 \\pmod{2}.\n",
    "$$\n",
    "\n",
    "\n",
    "## 4. Graphical Representation (Tanner Graph)\n",
    "\n",
    "An LDPC code is naturally represented by a **bipartite graph**:\n",
    "\n",
    "* **Variable nodes (VNs)**: Correspond to codeword bits.\n",
    "* **Check nodes (CNs)**: Correspond to parity-check equations.\n",
    "\n",
    "An edge connects variable node $v_j$ to check node $c_i$ if $H_{i,j} = 1$.\n",
    "\n",
    "**Advantages of Tanner graph**:\n",
    "\n",
    "* Makes iterative decoding (belief propagation) intuitive.\n",
    "* Sparsity ensures tractable complexity.\n",
    "\n",
    "\n",
    "\n",
    "## 5. Encoding\n",
    "\n",
    "Although defined by the parity-check matrix $H$, one often needs a **generator matrix** $G$ such that:\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\mathbf{u} G\n",
    "$$\n",
    "\n",
    "where $\\mathbf{u}$ is the message vector.\n",
    "\n",
    "Obtaining $G$ from $H$:\n",
    "\n",
    "* Systematic form: $H = [A \\mid I]$.\n",
    "* Solve $A \\cdot \\mathbf{m}^{\\top} = \\mathbf{p}^{\\top}$ for parity bits.\n",
    "\n",
    "For large, random LDPC codes, efficient encoding can be non-trivial, requiring special constructions (e.g., approximate lower triangular forms).\n",
    "\n",
    "\n",
    "## 6. Decoding Algorithms\n",
    "\n",
    "LDPC codes are typically decoded using **iterative message-passing algorithms** on their Tanner graphs.\n",
    "\n",
    "**6.1. Sum-Product Algorithm (Belief Propagation)**\n",
    "\n",
    "* Messages are log-likelihood ratios (LLRs).\n",
    "* Iteratively update messages between variable and check nodes.\n",
    "\n",
    "**Outline**:\n",
    "\n",
    "1. Initialize variable node LLRs with channel observations.\n",
    "2. For each edge, compute check-to-variable messages using parity-check constraints.\n",
    "3. For each edge, compute variable-to-check messages using incoming messages from other checks.\n",
    "4. Update variable node beliefs.\n",
    "5. Check for parity-check satisfaction.\n",
    "\n",
    "**Mathematical Form**:\n",
    "\n",
    "For a BIAWGN channel with noise variance $\\sigma^2$:\n",
    "\n",
    "* Input LLR: $L_i^{(0)} = \\frac{2 y_i}{\\sigma^2}$.\n",
    "\n",
    "Check-to-variable message:\n",
    "\n",
    "$$\n",
    "L_{c \\to v} = 2 \\tanh^{-1} \\left( \\prod_{v' \\in N(c) \\setminus v} \\tanh\\left( \\frac{L_{v' \\to c}}{2} \\right) \\right)\n",
    "$$\n",
    "\n",
    "Variable-to-check message:\n",
    "\n",
    "$$\n",
    "L_{v \\to c} = L_i^{(0)} + \\sum_{c' \\in N(v) \\setminus c} L_{c' \\to v}\n",
    "$$\n",
    "\n",
    "**6.2. Min-Sum Approximation**\n",
    "\n",
    "To reduce complexity:\n",
    "\n",
    "$$\n",
    "L_{c \\to v} \\approx \\min_{v' \\in N(c) \\setminus v} |L_{v' \\to c}| \\times \\prod \\text{sign}(L_{v' \\to c})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 7. Performance and Capacity Approaching Property\n",
    "\n",
    "LDPC codes can approach the Shannon capacity of various channels under belief-propagation decoding.\n",
    "\n",
    "* For BIAWGN channels, LDPC ensembles can be designed to operate close to capacity.\n",
    "* Threshold behavior: There exists an SNR threshold below which decoding fails with high probability.\n",
    "\n",
    "\n",
    "\n",
    "## 8. Design Considerations\n",
    "\n",
    "**Regular LDPC codes**:\n",
    "\n",
    "* Simple design.\n",
    "* Uniform weight distribution.\n",
    "* Good performance at moderate block lengths.\n",
    "\n",
    "**Irregular LDPC codes**:\n",
    "\n",
    "* Nodes with varying degrees.\n",
    "* Degree distributions optimized to improve convergence.\n",
    "\n",
    "**Degree distribution polynomials**:\n",
    "\n",
    "* Variable-node edge perspective: $\\lambda(x)$.\n",
    "* Check-node edge perspective: $\\rho(x)$.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\lambda(x) = \\sum_{i} \\lambda_i x^{i-1}, \\quad \\rho(x) = \\sum_{i} \\rho_i x^{i-1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 9. Density Evolution and Threshold Analysis\n",
    "\n",
    "**Density Evolution (DE)** is a powerful analytical method to evaluate asymptotic performance:\n",
    "\n",
    "* Tracks the distribution of messages (LLRs) passed in the graph over iterations.\n",
    "* Finds the **threshold**: the worst channel parameter where decoding still succeeds with vanishing error as block length → ∞.\n",
    "\n",
    "For BEC (Binary Erasure Channel), DE is exact:\n",
    "\n",
    "$$\n",
    "x^{(l+1)} = \\epsilon \\lambda\\left(1 - \\rho\\left(1 - x^{(l)}\\right)\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $x^{(l)}$: erasure probability after $l$ iterations.\n",
    "* $\\epsilon$: channel erasure probability.\n",
    "\n",
    "Threshold is found as the maximum $\\epsilon$ such that $x^{(\\infty)} = 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b5d77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Detailed Derivation of Sum-Product Algorithm (Belief Propagation) on Tanner Graph\n",
    "\n",
    "### 1.1 Setup\n",
    "\n",
    "Consider transmission over a **Binary-Input Additive White Gaussian Noise (BIAWGN) channel**.\n",
    "Transmitted codeword bit $c_i \\in \\{0,1\\}$ is BPSK mapped to $x_i \\in \\{+1,-1\\}$:\n",
    "\n",
    "$$\n",
    "x_i = 1 - 2c_i\n",
    "$$\n",
    "\n",
    "Received signal:\n",
    "\n",
    "$$\n",
    "y_i = x_i + n_i, \\quad n_i \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Initial LLR Computation\n",
    "\n",
    "The **channel LLR** for bit $i$ is:\n",
    "\n",
    "$$\n",
    "L_i^{(0)} = \\log \\frac{P(c_i=0|y_i)}{P(c_i=1|y_i)} \n",
    "$$\n",
    "\n",
    "For AWGN channel with BPSK signaling:\n",
    "\n",
    "$$\n",
    "L_i^{(0)} = \\frac{2y_i}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "This is the **input message** from the channel to each variable node.\n",
    "\n",
    "\n",
    "### 1.3 Message Passing in Tanner Graph\n",
    "\n",
    "The **Tanner graph** is bipartite:\n",
    "\n",
    "* Variable nodes $v$\n",
    "* Check nodes $c$\n",
    "\n",
    "We pass **LLR messages** along edges.\n",
    "\n",
    "**Notation**:\n",
    "\n",
    "* $L_{v \\to c}$: message from variable node $v$ to check node $c$\n",
    "* $L_{c \\to v}$: message from check node $c$ to variable node $v$\n",
    "\n",
    "\n",
    "### 1.4 Update Equations\n",
    "\n",
    "#### (a) Variable Node Update\n",
    "\n",
    "Variable node $v$ combines all incoming check node messages except from $c$:\n",
    "\n",
    "$$\n",
    "L_{v \\to c} = L_v^{(0)} + \\sum_{c' \\in N(v) \\setminus c} L_{c' \\to v}\n",
    "$$\n",
    "\n",
    "* $L_v^{(0)}$ is the **channel LLR**.\n",
    "* Sum over other connected check nodes.\n",
    "\n",
    "\n",
    "#### (b) Check Node Update\n",
    "\n",
    "Check nodes impose parity-check constraints:\n",
    "\n",
    "$$\n",
    "\\sum_{v \\in N(c)} c_v = 0 \\mod 2\n",
    "$$\n",
    "\n",
    "For binary variables, the **probability** that the parity-check is satisfied given incoming messages can be derived using factor graphs. The check-to-variable message in LLR form is:\n",
    "\n",
    "$$\n",
    "L_{c \\to v} = 2 \\tanh^{-1} \\left( \\prod_{v' \\in N(c) \\setminus v} \\tanh\\left( \\frac{L_{v' \\to c}}{2} \\right) \\right)\n",
    "$$\n",
    "\n",
    "**Derivation Sketch**:\n",
    "\n",
    "* The probability of even parity is:\n",
    "\n",
    "$$\n",
    "P(\\text{even}) = \\frac{1}{2} \\left(1 + \\prod_{v' \\neq v} (1 - 2p_{v'}) \\right)\n",
    "$$\n",
    "\n",
    "* Converting probabilities to LLRs and applying log-sum-exp identities leads to the hyperbolic tangent formula.\n",
    "\n",
    "\n",
    "### 1.5 Decision\n",
    "\n",
    "After many iterations, variable node beliefs are:\n",
    "\n",
    "$$\n",
    "L_v = L_v^{(0)} + \\sum_{c \\in N(v)} L_{c \\to v}\n",
    "$$\n",
    "\n",
    "Decision:\n",
    "\n",
    "$$\n",
    "\\hat{c}_v = \\begin{cases}\n",
    "0 & \\text{if } L_v \\geq 0 \\\\\n",
    "1 & \\text{if } L_v < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 2. Min-Sum Approximation\n",
    "\n",
    "To reduce complexity of evaluating $\\tanh^{-1}$ and products, the **min-sum approximation** is used.\n",
    "\n",
    "Original:\n",
    "\n",
    "$$\n",
    "L_{c \\to v} = 2 \\tanh^{-1} \\left( \\prod_{v' \\in N(c) \\setminus v} \\tanh \\frac{L_{v' \\to c}}{2} \\right)\n",
    "$$\n",
    "\n",
    "Approximation:\n",
    "\n",
    "$$\n",
    "L_{c \\to v} \\approx \\min_{v' \\in N(c) \\setminus v} |L_{v' \\to c}| \\times \\prod_{v'} \\text{sign}(L_{v' \\to c})\n",
    "$$\n",
    "\n",
    "Rationale:\n",
    "\n",
    "* $\\tanh^{-1}(x) \\approx x$ for small $x$.\n",
    "* Products of $\\tanh$ approximated by minimum in log-domain.\n",
    "\n",
    "\n",
    "\n",
    "## 3. Practical Example of a Small LDPC Code\n",
    "\n",
    "### 3.1 Small Parity-Check Matrix\n",
    "\n",
    "Consider:\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "1 & 1 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 1 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 1 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* $n = 6$ (codeword length)\n",
    "* $m = 3$ (number of parity-checks)\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Tanner Graph\n",
    "\n",
    "* **Variable nodes**: v1 to v6\n",
    "* **Check nodes**: c1 to c3\n",
    "\n",
    "**Edges**:\n",
    "\n",
    "* c1 connects v1, v2, v4\n",
    "* c2 connects v2, v3, v5\n",
    "* c3 connects v1, v3, v6\n",
    "\n",
    "\n",
    "### 3.3 Parity Constraints\n",
    "\n",
    "Codewords $\\mathbf{c}$ satisfy:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "c_1 + c_2 + c_4 = 0 \\mod 2 \\\\\n",
    "c_2 + c_3 + c_5 = 0 \\mod 2 \\\\\n",
    "c_1 + c_3 + c_6 = 0 \\mod 2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 4. Encoding Example\n",
    "\n",
    "LDPC codes are often defined by $H$, but to **encode**, we need $G$.\n",
    "\n",
    "**Method**: Convert $H$ to systematic form:\n",
    "\n",
    "Suppose $H = [A \\mid I]$ for suitable column permutations.\n",
    "\n",
    "For example, rearrange:\n",
    "\n",
    "$$\n",
    "H' = \\begin{bmatrix}\n",
    "1 & 0 & 1 & 0 & 0 & 1 \\\\\n",
    "0 & 1 & 1 & 0 & 1 & 0 \\\\\n",
    "1 & 1 & 0 & 1 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Systematic form:\n",
    "\n",
    "$$\n",
    "H' = [A \\mid I_3], \\quad A = \\text{parity generation matrix}\n",
    "$$\n",
    "\n",
    "Encoding:\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = A \\cdot \\mathbf{m}^{\\top} \\pmod{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = [\\mathbf{m} \\mid \\mathbf{p}]\n",
    "$$\n",
    "\n",
    "\n",
    "## 5. Irregular LDPC Code Construction\n",
    "\n",
    "### 5.1 Degree Distribution\n",
    "\n",
    "Irregular LDPC codes use variable node and check node **degree distributions**.\n",
    "\n",
    "**Edge perspective polynomials**:\n",
    "\n",
    "$$\n",
    "\\lambda(x) = \\sum_{i} \\lambda_i x^{i-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rho(x) = \\sum_{i} \\rho_i x^{i-1}\n",
    "$$\n",
    "\n",
    "* $\\lambda_i$: fraction of edges connected to degree $i$ variable nodes.\n",
    "* $\\rho_i$: fraction of edges connected to degree $i$ check nodes.\n",
    "\n",
    "\n",
    "\n",
    "### 5.2 Example Degree Distribution\n",
    "\n",
    "Design for BEC:\n",
    "\n",
    "$$\n",
    "\\lambda(x) = 0.5x + 0.5x^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rho(x) = x^5\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* Half of edges connect to degree 2 variable nodes.\n",
    "* Half connect to degree 3 variable nodes.\n",
    "* All check nodes have degree 6.\n",
    "\n",
    "\n",
    "### 5.3 PEG (Progressive Edge Growth) Construction\n",
    "\n",
    "One practical construction method:\n",
    "\n",
    "1. Fix desired degrees for each node from the distribution.\n",
    "2. Grow the Tanner graph edge by edge:\n",
    "\n",
    "   * Connect variable node with smallest degree deficit.\n",
    "   * Choose check node to minimize cycles.\n",
    "\n",
    "PEG ensures:\n",
    "\n",
    "* Desired degree distributions.\n",
    "* Large girth (few short cycles).\n",
    "\n",
    "\n",
    "\n",
    "## Summary of Derived Equations\n",
    "\n",
    "**Variable Node Update**:\n",
    "\n",
    "$$\n",
    "L_{v \\to c} = L_v^{(0)} + \\sum_{c' \\in N(v) \\setminus c} L_{c' \\to v}\n",
    "$$\n",
    "\n",
    "**Check Node Update (Sum-Product)**:\n",
    "\n",
    "$$\n",
    "L_{c \\to v} = 2 \\tanh^{-1} \\left( \\prod_{v' \\in N(c) \\setminus v} \\tanh\\left(\\frac{L_{v' \\to c}}{2}\\right) \\right)\n",
    "$$\n",
    "\n",
    "**Check Node Update (Min-Sum)**:\n",
    "\n",
    "$$\n",
    "L_{c \\to v} \\approx \\min_{v' \\in N(c) \\setminus v} |L_{v' \\to c}| \\times \\prod \\text{sign}(L_{v' \\to c})\n",
    "$$\n",
    "\n",
    "**Belief Update**:\n",
    "\n",
    "$$\n",
    "L_v = L_v^{(0)} + \\sum_{c \\in N(v)} L_{c \\to v}\n",
    "$$\n",
    "\n",
    "**Decision Rule**:\n",
    "\n",
    "$$\n",
    "\\hat{c}_v = \\begin{cases}\n",
    "0 & \\text{if } L_v \\geq 0 \\\\\n",
    "1 & \\text{if } L_v < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c1b46",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# simulation of LDPC decoding\n",
    "\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "* Small (toy) LDPC code to keep it understandable.\n",
    "* Binary code over GF(2).\n",
    "* AWGN channel.\n",
    "* BPSK modulation.\n",
    "* Sum-Product Algorithm with LLRs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342ccb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded bits: [0 0 0 0 0 0]\n",
      "Syndrome (should be all zero): [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "# Define LDPC Parity-Check Matrix\n",
    "H = np.array([\n",
    "    [1, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 0, 1, 0],\n",
    "    [1, 0, 1, 0, 0, 1]\n",
    "])\n",
    "# Generate a Codeword\n",
    "# For simplicity, use all-zeros codeword (valid under any linear code):\n",
    "n = H.shape[1]\n",
    "codeword = np.zeros(n, dtype=int)  # All-zero codeword\n",
    "# BPSK Modulation and AWGN Channel\n",
    "def bpsk_mod(bits):\n",
    "    return 1 - 2 * bits\n",
    "\n",
    "def awgn_channel(x, snr_db):\n",
    "    snr_linear = 10**(snr_db/10)\n",
    "    sigma = np.sqrt(1/(2*snr_linear))\n",
    "    noise = np.random.randn(len(x)) * sigma\n",
    "    return x + noise\n",
    "\n",
    "# Simulation parameters\n",
    "snr_db = 2\n",
    "x_tx = bpsk_mod(codeword)\n",
    "y_rx = awgn_channel(x_tx, snr_db)\n",
    "\n",
    "# Compute Channel LLRs \n",
    "# for BIAWNG LLR = 2y/sigma^2\n",
    "def compute_llrs(y, snr_db):\n",
    "    snr_linear = 10**(snr_db/10)\n",
    "    sigma2 = 1/(2*snr_linear)\n",
    "    return 2 * y / sigma2\n",
    "\n",
    "llr_channel = compute_llrs(y_rx, snr_db)\n",
    "\n",
    "# Sum-Product Decoding on Tanner Graph\n",
    "# implement message-passing over a fixed number of iterations.\n",
    "num_checks, num_vars = H.shape\n",
    "max_iters = 10\n",
    "\n",
    "# Messages on edges\n",
    "m_v_to_c = H.astype(float) * llr_channel[None, :]  # Initialize with channel LLRs\n",
    "m_c_to_v = np.zeros_like(m_v_to_c)\n",
    "\n",
    "for it in range(max_iters):\n",
    "    # Check node update\n",
    "    for c in range(num_checks):\n",
    "        for v in range(num_vars):\n",
    "            if H[c, v]:\n",
    "                neighbors = np.where(H[c, :] == 1)[0]\n",
    "                neighbors = neighbors[neighbors != v]\n",
    "                prod = 1.0\n",
    "                for v_prime in neighbors:\n",
    "                    val = np.tanh(0.5 * m_v_to_c[c, v_prime])\n",
    "                    prod *= val\n",
    "                msg = 2 * np.arctanh(prod + 1e-12)\n",
    "                m_c_to_v[c, v] = msg\n",
    "\n",
    "    # Variable node update\n",
    "    for v in range(num_vars):\n",
    "        for c in range(num_checks):\n",
    "            if H[c, v]:\n",
    "                neighbors = np.where(H[:, v] == 1)[0]\n",
    "                neighbors = neighbors[neighbors != c]\n",
    "                msg_sum = llr_channel[v]\n",
    "                for c_prime in neighbors:\n",
    "                    msg_sum += m_c_to_v[c_prime, v]\n",
    "                m_v_to_c[c, v] = msg_sum\n",
    "\n",
    "# Final Beliefs and Hard Decisions\n",
    "beliefs = llr_channel.copy()\n",
    "for v in range(num_vars):\n",
    "    neighbors = np.where(H[:, v] == 1)[0]\n",
    "    beliefs[v] += m_c_to_v[neighbors, v].sum()\n",
    "\n",
    "decoded_bits = (beliefs < 0).astype(int)\n",
    "\n",
    "# Check Parity\n",
    "syndrome = H @ decoded_bits % 2\n",
    "print(\"Decoded bits:\", decoded_bits)\n",
    "print(\"Syndrome (should be all zero):\", syndrome)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2620a39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "badf2831",
   "metadata": {},
   "source": [
    "BCH, RS, OFEC\n",
    "FEC for optical \n",
    "\n",
    "Designing DSP blocks for optical comm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
